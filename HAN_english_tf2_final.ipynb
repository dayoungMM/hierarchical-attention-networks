{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dayoung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dayoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dayoung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "# import sys\n",
    "# from IPython.display import display, HTML\n",
    "# import urllib\n",
    "# import gzip\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import pip\n",
    "# import theano\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from tensorflow.keras import initializers as initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.engine.topology import Layer\n",
    "# from tensorflow.keras.utils import plot_model, np_utils\n",
    "# from tensorflow.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(review,stopWords):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.sent_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' . '\n",
    "    \n",
    "    return returnString, idx_list\n",
    "\n",
    "\n",
    "def split_df(dataframe, column_name, training_split = 0.6, validation_split = 0.2, test_split = 0.2):\n",
    "    \"\"\"\n",
    "    Splits a pandas dataframe into trainingset, validationset and testset in specified ratio.\n",
    "    All sets are balanced, which means they have the same ratio for each categorie as the full set.\n",
    "    Input:   dataframe        - Pandas Dataframe, should include a column for data and one for categories\n",
    "             column_name      - Name of dataframe column which contains the categorical output values\n",
    "             training_split   - from ]0,1[, default = 0.6\n",
    "             validation_split - from ]0,1[, default = 0.2        \n",
    "             test_split       - from ]0,1[, default = 0.2\n",
    "                                Sum of all splits need to be 1\n",
    "    Output:  train            - Pandas DataFrame of trainset\n",
    "             validation       - Pandas DataFrame of validationset\n",
    "             test             - Pandas DataFrame of testset\n",
    "    \"\"\"\n",
    "    if training_split + validation_split + test_split != 1.0:\n",
    "        raise ValueError('Split paramter sum should be 1.0')\n",
    "        \n",
    "    total = len(dataframe.index)\n",
    " \n",
    "    train = dataframe.reset_index().groupby(column_name).apply(lambda x: x.sample(frac=training_split))\\\n",
    "    .reset_index(drop=True).set_index('index')\n",
    "    train = train.sample(frac=1)\n",
    "    temp_df = dataframe.drop(train.index)\n",
    "    validation = temp_df.reset_index().groupby(column_name)\\\n",
    "    .apply(lambda x: x.sample(frac=validation_split/(test_split+validation_split)))\\\n",
    "           .reset_index(drop=True).set_index('index')\n",
    "    validation = validation.sample(frac=1)\n",
    "    test = temp_df.drop(validation.index)\n",
    "    test = test.sample(frac=1)\n",
    "    \n",
    "    print('Total: ', len(dataframe))\n",
    "    print('Training: ', len(train), ', Percentage: ', len(train)/len(dataframe))\n",
    "    print('Validation: ', len(validation), ', Percentage: ', len(validation)/len(dataframe))\n",
    "    print('Test:', len(test), ', Percentage: ', len(test)/len(dataframe))\n",
    "\n",
    "    return train, validation, test\n",
    "\n",
    "def wordToSeq(text,word_index,max_sentences,max_words,max_features):\n",
    "    \"\"\"\n",
    "    Converts a string to a numpy matrix where each word is tokenized.\n",
    "    Arrays are zero-padded to max_sentences and max_words length.\n",
    "    \n",
    "    Input:    text           - string of sentences\n",
    "              word_index     - trained word_index\n",
    "              max_sentences  - maximum number of sentences allowed per document for HAN\n",
    "              max_words      - maximum number of words in each sentence for HAN\n",
    "              max_features   - maximum number of unique words to be tokenized\n",
    "    Output:   data           - Numpy Matrix of size [max_sentences x max_words]\n",
    "    \"\"\"\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    data = np.zeros((max_sentences, max_words), dtype='int32')\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< max_sentences:\n",
    "            wordTokens = tokenize.word_tokenize(sent.rstrip('.'))\n",
    "            wordTokens = [w for w in wordTokens]\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k<max_words and word_index[word]<max_features:\n",
    "                        data[j,k] = word_index[word]\n",
    "                        k=k+1\n",
    "                except:\n",
    "                    pass\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      5  I have bought several of the Vitality canned d...\n",
       "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2      4  This is a confection that has been around a fe...\n",
       "3      2  If you are looking for the secret ingredient i...\n",
       "4      5  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('dataset/Reviews.csv')\n",
    "data_df = data_df[['Score','Text']]\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in each sentence:  19\n",
      "Average number of sentences in each document:  5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute average number of words in each sentence and average number of sentences in each document.\n",
    "\"\"\"\n",
    "n_sent = 0\n",
    "n_words = 0\n",
    "for i in range(data_df.shape[0]):\n",
    "    sent = tokenize.sent_tokenize(data_df.loc[i,'Text'])\n",
    "    for satz in sent:\n",
    "        n_words += len(tokenize.word_tokenize(satz))\n",
    "    n_sent += len(sent)\n",
    "    \n",
    "print(\"Average number of words in each sentence: \",round(n_words/n_sent))\n",
    "print(\"Average number of sentences in each document: \", round(n_sent/data_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 200000 # maximum number of unique words that should be included in the tokenized word index\n",
    "MAX_SENTENCE_NUM = 40 # maximum number of sentences in one document\n",
    "MAX_WORD_NUM = 50     # maximum number of words in each sentence\n",
    "EMBED_SIZE = 100      # vector size of word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568454 of 568454 articles cleaned.\r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Category'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d10faf5f733b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdata_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdata_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcategoryToCode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Category'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cleans raw data using the cleanString() function from above.\n",
    "English stopwords are used from nltk library.\n",
    "Cleaned dataset is saved in 'data_cleaned' pandas dataframe.\n",
    "Labels are converted to numbers,\n",
    "\"\"\"\n",
    "# articles = []\n",
    "# n = data_df['Text'].shape[0]\n",
    "# col_number = data_df.columns.get_loc('Text')\n",
    "# stopWords = set(stopwords.words('english'))\n",
    "# data_cleaned = data_df.copy()\n",
    "# for i in range(n):\n",
    "#     temp_string,idx_string = cleanString(data_df.iloc[i,col_number],stopWords)\n",
    "#     articles.append(temp_string)\n",
    "#     print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
    "    \n",
    "# data_cleaned.loc[:,'Text'] = pd.Series(articles,index=data_df.index)\n",
    "# data_cleaned.loc[:,'Category'] = pd.Categorical(data_cleaned.Category)\n",
    "# data_cleaned['Code'] = data_cleaned.Category.cat.codes\n",
    "# categoryToCode = dict( enumerate(data_cleaned['Category'].cat.categories))\n",
    "\n",
    "# data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean String\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleanString(review,stopWords):\n",
    "    \"\"\"\n",
    "    Cleans input string using set rules.\n",
    "    Cleaning rules:         Every word is lemmatized and lowercased. Stopwords and non alpha-numeric words are removed.\n",
    "                            Each sentence ends with a period.\n",
    "    Input:   review       - string(in sentence structure)\n",
    "             stopWords    - set of strings which should be removed from review\n",
    "    Output:  returnString - cleaned input string\n",
    "             idx_list     - list of lists, one list is equal to one sentence. In every list are the index\n",
    "                            of each word as they appeared in the non cleaned sentence\n",
    "                            e.g. nonCleaned = \"This is a test.\" -> cleaned = \"This test.\" -> cleaned_index = [[0,3]]\n",
    "    \"\"\"\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.sent_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' . '\n",
    "    \n",
    "    return returnString, idx_list\n",
    "\n",
    "print(\"clenString example:\")\n",
    "print(cleanString(df_review['Text'][0],['a','the'])[0])\n",
    "\n",
    "# df_review['Text']=df_review['Text'].apply(lambda x : cleanString(x,['a','the'])[0])\n",
    "\n",
    "## cleanString 한 결과 저장해놓은 것\n",
    "# df_review = pd.read_csv('dataset/df_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:10: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<ipython-input-7-b9b57f0d9043>:10: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  s = ' '.join([word.strip(string.punctuation) for word in s.split() if word.strip(string.punctuation) is not \"\"])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Using the keras Tokenizer class a word index is built.\n",
    "The most 'MAX_FEATURES' used words are tokenized to a number.\n",
    "this dictionary is saved in word_index\n",
    "\"\"\"\n",
    "texts = []\n",
    "n = data_cleaned['Text'].shape[0]\n",
    "for i in range(n):\n",
    "    s = data_cleaned['Text'].iloc[i]\n",
    "    s = ' '.join([word.strip(string.punctuation) for word in s.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    texts.append(s)\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES,lower=True, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Total absent words are 66396 which is 52.37 % of total words\n",
      "Words with 0 or less mentions 0 which is 0.00 % of total words\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A pre-trained word to vector is used from GloVe by Pennington et. al.\n",
    "Source: https://nlp.stanford.edu/projects/glove/\n",
    "The data was trained on wikipedia articles. Each word is described by a 100d vector.\n",
    "\"\"\"\n",
    "\n",
    "# Load word vectors from pre-trained dataset\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(os.getcwd(), 'dataset/glove.6B.100d.txt'),encoding='UTF-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Search words in our word index in the pre-trained dataset\n",
    "# Create an embedding matrix for our bbc dataset\n",
    "min_wordCount = 0\n",
    "absent_words = 0\n",
    "small_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "word_counts = tokenizer.word_counts\n",
    "for word, i in word_index.items():\n",
    "    if word_counts[word] > min_wordCount:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            absent_words += 1\n",
    "    else:\n",
    "        small_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)),\n",
    "      '% of total words')\n",
    "print('Words with '+str(min_wordCount)+' or less mentions', small_words, 'which is', \"%0.2f\" % (small_words * 100 / len(word_index)),\n",
    "      '% of total words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
       "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
       "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
       "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
       "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
       "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
       "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
       "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
       "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
       "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
       "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
       "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
       "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
       "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
       "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
       "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
       "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
       "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
       "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
       "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display(example_df_embedding)\n",
    "embedding_matrix[word_index['great']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  568454\n",
      "Training:  454763 , Percentage:  0.7999996481685413\n",
      "Validation:  56846 , Percentage:  0.10000105549437598\n",
      "Test: 56845 , Percentage:  0.09999929633708268\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split Pandas Dataframe into train, validation and testset.\n",
    "Convert data to keras conforming form\n",
    "\"\"\"\n",
    "# print(categoryToCode)\n",
    "train, validation, test = split_df(data_cleaned, 'Score',0.8,0.1,0.1)\n",
    "\n",
    "#Training\n",
    "paras = []\n",
    "for i in range(train['Text'].shape[0]):\n",
    "    sequence = wordToSeq(train['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
    "    paras.append(sequence)\n",
    "x_train = np.array(paras)\n",
    "# y_train = to_categorical(train['Score'],categoryToCode)\n",
    "y_train = pd.get_dummies(train['Score'])\n",
    "\n",
    "#Validation\n",
    "paras = []\n",
    "for i in range(validation['Text'].shape[0]):\n",
    "    sequence = wordToSeq(validation['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
    "    paras.append(sequence)\n",
    "x_val = np.array(paras)\n",
    "# y_val = to_categorical(validation['Score'],categoryToCode)\n",
    "y_val = pd.get_dummies(validation['Score'])\n",
    "#Test\n",
    "paras = []\n",
    "for i in range(test['Text'].shape[0]):\n",
    "    sequence = wordToSeq(test['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
    "    paras.append(sequence)\n",
    "x_test = np.array(paras)\n",
    "# y_test = to_categorical(test['Score'],categoryToCode)\n",
    "y_test = pd.get_dummies(test['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(\"/gpu:0\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n",
    "    - Yang et. al.\n",
    "    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "    Theano backend\n",
    "    \"\"\"\n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self._trainable_weights = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input (InputLayer)      [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "word_embedding (Embedding)   (None, 50, 100)           12677700  \n",
      "_________________________________________________________________\n",
      "word_gru (Bidirectional)     (None, 50, 100)           45600     \n",
      "_________________________________________________________________\n",
      "word_dense (Dense)           (None, 50, 100)           10100     \n",
      "_________________________________________________________________\n",
      "word_attention (AttentionLay [(None, 100), (None, 50,  10200     \n",
      "=================================================================\n",
      "Total params: 12,743,600\n",
      "Trainable params: 65,900\n",
      "Non-trainable params: 12,677,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      [(None, 40, 50)]          0         \n",
      "_________________________________________________________________\n",
      "sent_linking (TimeDistribute (None, 40, 100)           12743600  \n",
      "_________________________________________________________________\n",
      "sent_gru (Bidirectional)     (None, 40, 100)           45600     \n",
      "_________________________________________________________________\n",
      "sent_dense (Dense)           (None, 40, 100)           10100     \n",
      "_________________________________________________________________\n",
      "sent_attention (AttentionLay [(None, 100), (None, 40,  10200     \n",
      "_________________________________________________________________\n",
      "sent_dropout (Dropout)       (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 12,810,005\n",
      "Trainable params: 132,305\n",
      "Non-trainable params: 12,677,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create Keras functional model for hierarchical attention network\n",
    "\"\"\"\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    embedding_layer = Embedding(len(word_index) + 1,EMBED_SIZE,weights=[embedding_matrix], \n",
    "                                input_length=MAX_WORD_NUM, trainable=False,name='word_embedding')\n",
    "\n",
    "    # Words level attention model\n",
    "    word_input = Input(shape=(MAX_WORD_NUM,), dtype='int32',name='word_input')\n",
    "    word_sequences = embedding_layer(word_input)\n",
    "    word_gru = Bidirectional(GRU(50, return_sequences=True),name='word_gru')(word_sequences)\n",
    "    word_dense = Dense(100, activation='relu', name='word_dense')(word_gru) \n",
    "    word_att,word_coeffs = AttentionLayer(EMBED_SIZE,True,name='word_attention')(word_dense)\n",
    "    wordEncoder = Model(inputs = word_input,outputs = word_att)\n",
    "\n",
    "    # Sentence level attention model\n",
    "    sent_input = Input(shape=(MAX_SENTENCE_NUM,MAX_WORD_NUM), dtype='int32',name='sent_input')\n",
    "    sent_encoder = TimeDistributed(wordEncoder,name='sent_linking')(sent_input)\n",
    "    sent_gru = Bidirectional(GRU(50, return_sequences=True),name='sent_gru')(sent_encoder)\n",
    "    sent_dense = Dense(100, activation='relu', name='sent_dense')(sent_gru) \n",
    "    sent_att,sent_coeffs = AttentionLayer(EMBED_SIZE,return_coefficients=True,name='sent_attention')(sent_dense)\n",
    "    sent_drop = Dropout(0.5,name='sent_dropout')(sent_att)\n",
    "    preds = Dense(5, activation='softmax',name='output')(sent_drop)\n",
    "\n",
    "    # Model compile\n",
    "    model = Model(sent_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "    print(wordEncoder.summary())\n",
    "    print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9096/9096 [==============================] - 6839s 752ms/step - loss: 0.7776 - acc: 0.7142 - val_loss: 0.7456 - val_acc: 0.7244\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeNUlEQVR4nO3de3RV5b3u8e8jV0GUgNEqIMTWlgBSwCXabW29VDfaqnglVrvF3ZZdd62trQ6x7T51u+s+9nLE0ZZe8Ki1+1iQRlH2qR68gW1H0RIsIhcvEXUQ8IJUFAVU8Hf+WDN0JqxgJslkJeT5jLFG1nzfd7553zBYT+Z8Z+ZURGBmZtZae5V7AGZm1rk4OMzMLBMHh5mZZeLgMDOzTBwcZmaWiYPDzMwycXCY5UjSryV9v5VtX5D0mbb2Y5Y3B4eZmWXi4DAzs0wcHNblJaeIrpS0VNLbkm6WdKCk+yRtlPSgpIpU+9MlLZe0QdICSdWpurGSHk/2uwPo3ex7fU7SkmTfP0savYtj/rKkekl/kzRX0sFJuSRNk/SqpDclPSlpVFJ3qqQVydjWSLpil35g1uU5OMyKzgZOAj4KnAbcB3wbqKT4/+QyAEkfBWYC30jq7gX+W1JPST2Bu4H/AgYAv0v6Jdl3LHAL8C/AQOBXwFxJvbIMVNIJwP8EzgMOAl4EZiXVJwOfSuaxX9JmfVJ3M/AvEdEPGAU8nOX7mjVycJgV/TQiXomINcAfgcci4q8RsQWYA4xN2k0Cfh8RD0TEe8CPgb2BfwCOBnoAN0bEexFRCyxKfY8pwK8i4rGI2BYRtwHvJPtlcQFwS0Q8HhHvAFcDn5A0DHgP6AcMBxQRKyPipWS/94ARkvaNiNcj4vGM39cMcHCYNXol9X5zie19kvcHU/wNH4CIeB9YDQxK6tZE0zuHvph6PxT4VnKaaoOkDcCQZL8smo/hLYpHFYMi4mHgZ8B04FVJMyTtmzQ9GzgVeFHSI5I+kfH7mgEODrOs1lIMAKC4pkDxw38N8BIwKClrdEjq/Wrguojon3r1iYiZbRxDX4qnvtYARMRPIuIIYATFU1ZXJuWLIuIM4ACKp9RmZ/y+ZoCDwyyr2cBnJZ0oqQfwLYqnm/4MLAS2ApdJ6iHpLGB8at+bgK9IOipZxO4r6bOS+mUcw0zgYkljkvWR/6R4au0FSUcm/fcA3ga2AO8nazAXSNovOcX2JvB+G34O1oU5OMwyiIingQuBnwKvUVxIPy0i3o2Id4GzgMnA3yiuh9yV2rcO+DLFU0mvA/VJ26xjeBD4N+BOikc5HwZqkup9KQbU6xRPZ60HfpTUfQF4QdKbwFcorpWYZSY/yMnMzLLwEYeZmWXi4DAzs0wcHGZmlomDw8zMMule7gHsDvvvv38MGzas3MMwM+tUFi9e/FpEVDYv7xLBMWzYMOrq6so9DDOzTkXSi6XKfarKzMwycXCYmVkmDg4zM8ukS6xxlPLee+/R0NDAli1byj2UPULv3r0ZPHgwPXr0KPdQzCxnXTY4Ghoa6NevH8OGDaPpzUwtq4hg/fr1NDQ0UFVVVe7hmFnOuuypqi1btjBw4ECHRjuQxMCBA330ZtZFdNngABwa7cg/S7Ouo0sHh5mZZefgKJMNGzbw85//PPN+p556Khs2bGj/AZmZtZKDo0xaCo6tW7fudL97772X/v375zQqM7MP1mWvqiq3qVOn8txzzzFmzBh69OhB7969qaio4KmnnuKZZ55h4sSJrF69mi1btvD1r3+dKVOmAH+/fcpbb73FKaecwic/+Un+/Oc/M2jQIO655x723nvvMs/MzPZ0Dg7g3/97OSvWvtmufY44eF++d9rIFuuvv/56li1bxpIlS1iwYAGf/exnWbZs2fbLWW+55RYGDBjA5s2bOfLIIzn77LMZOHBgkz6effZZZs6cyU033cR5553HnXfeyYUXXtiu8zAza87B0UGMHz++yd9A/OQnP2HOnDkArF69mmeffXaH4KiqqmLMmDEAHHHEEbzwwgu7a7hm1oU5OGCnRwa7S9++fbe/X7BgAQ8++CALFy6kT58+HHfccSX/RqJXr17b33fr1o3NmzfvlrGaWdfmxfEy6devHxs3bixZ98Ybb1BRUUGfPn146qmnePTRR3fz6MzMWuYjjjIZOHAgxxxzDKNGjWLvvffmwAMP3F43YcIEfvnLX1JdXc3HPvYxjj766DKO1MysKUVEuceQu0KhEM0f5LRy5Uqqq6vLNKI9k3+mZnsWSYsjotC83KeqzMwsEweHmZll4uAwM7NMHBxmZpZJrsEhaYKkpyXVS5paon6apCXJ6xlJG5Ly41PlSyRtkTQxqfu1pOdTdWPynIOZmTWV2+W4kroB04GTgAZgkaS5EbGisU1EXJ5q/zVgbFI+HxiTlA8A6oH7U91fGRG1eY3dzMxalucRx3igPiJWRcS7wCzgjJ20Px+YWaL8HOC+iNiUwxg7jX322QeAtWvXcs4555Rsc9xxx9H8suPmbrzxRjZt+vuP0rdpN7Os8gyOQcDq1HZDUrYDSUOBKuDhEtU17Bgo10lampzq6lViHyRNkVQnqW7dunXZR99BHXzwwdTW7vrBVvPg8G3azSyrjrI4XgPURsS2dKGkg4DDgXmp4quB4cCRwADgqlIdRsSMiChERKGysjKfUbfB1KlTmT59+vbta665hu9///uceOKJjBs3jsMPP5x77rlnh/1eeOEFRo0aBcDmzZupqamhurqaM888s8m9qi655BIKhQIjR47ke9/7HlC8ceLatWs5/vjjOf7444Hibdpfe+01AG644QZGjRrFqFGjuPHGG7d/v+rqar785S8zcuRITj75ZN8Ty6yLy/OWI2uAIantwUlZKTXAV0uUnwfMiYj3Ggsi4qXk7TuSbgWuaPNI75sKLz/Z5m6a+NDhcMr1LVZPmjSJb3zjG3z1q8Vpz549m3nz5nHZZZex77778tprr3H00Udz+umnt/g871/84hf06dOHlStXsnTpUsaNG7e97rrrrmPAgAFs27aNE088kaVLl3LZZZdxww03MH/+fPbff/8mfS1evJhbb72Vxx57jIjgqKOO4tOf/jQVFRW+fbuZNZHnEcci4DBJVZJ6UgyHuc0bSRoOVAALS/Sxw7pHchSCip+mE4Fl7Tvs3WPs2LG8+uqrrF27lieeeIKKigo+9KEP8e1vf5vRo0fzmc98hjVr1vDKK6+02Mcf/vCH7R/go0ePZvTo0dvrZs+ezbhx4xg7dizLly9nxYoVLXUDwJ/+9CfOPPNM+vbtyz777MNZZ53FH//4R8C3bzezpnI74oiIrZIupXiaqRtwS0Qsl3QtUBcRjSFSA8yKZjfNkjSM4hHLI826vl1SJSBgCfCVNg92J0cGeTr33HOpra3l5ZdfZtKkSdx+++2sW7eOxYsX06NHD4YNG1byduof5Pnnn+fHP/4xixYtoqKigsmTJ+9SP418+3YzS8t1jSMi7o2Ij0bEhyPiuqTsf6RCg4i4JiJ2+BuPiHghIgZFxPvNyk+IiMMjYlREXBgRb+U5hzxNmjSJWbNmUVtby7nnnssbb7zBAQccQI8ePZg/fz4vvvjiTvf/1Kc+xW9/+1sAli1bxtKlSwF488036du3L/vttx+vvPIK99133/Z9Wrqd+7HHHsvdd9/Npk2bePvtt5kzZw7HHntsO87WzPYUvq16GY0cOZKNGzcyaNAgDjroIC644AJOO+00Dj/8cAqFAsOHD9/p/pdccgkXX3wx1dXVVFdXc8QRRwDw8Y9/nLFjxzJ8+HCGDBnCMcccs32fKVOmMGHCBA4++GDmz5+/vXzcuHFMnjyZ8ePHA/ClL32JsWPH+rSUme3At1W3duOfqdmexbdVNzOzduHgMDOzTLp0cHSF03S7i3+WZl1Hlw2O3r17s379en/gtYOIYP369fTu3bvcQzGz3aDLXlU1ePBgGhoa2JPuY1VOvXv3ZvDgweUehpntBl02OHr06EFVVVW5h2Fm1ul02VNVZma2axwcZmaWiYPDzMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLBMHh5mZZeLgMDOzTBwcZmaWiYPDzMwycXCYmVkmDg4zM8vEwWFmZpnkGhySJkh6WlK9pKkl6qdJWpK8npG0ISk/PlW+RNIWSROTuipJjyV93iGpZ55zMDOzpnILDkndgOnAKcAI4HxJI9JtIuLyiBgTEWOAnwJ3JeXzU+UnAJuA+5PdfgBMi4iPAK8DX8xrDmZmtqM8jzjGA/URsSoi3gVmAWfspP35wMwS5ecA90XEJkmiGCS1Sd1twMT2G7KZmX2QPINjELA6td2QlO1A0lCgCni4RHUNfw+UgcCGiNjaij6nSKqTVOfHw5qZtZ+OsjheA9RGxLZ0oaSDgMOBeVk7jIgZEVGIiEJlZWU7DdPMzPIMjjXAkNT24KSslPRRRdp5wJyIeC/ZXg/0l9T4rPSd9WlmZjnIMzgWAYclV0H1pBgOc5s3kjQcqAAWluijybpHRAQwn+K6B8BFwD3tPG4zM9uJ3IIjWYe4lOJpppXA7IhYLulaSaenmtYAs5JQ2E7SMIpHLI806/oq4JuS6imuedyc0xTMzKwENfu83iMVCoWoq6sr9zDMzDoVSYsjotC8vKMsjpuZWSfh4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLJNfgkDRB0tOS6iVNLVE/TdKS5PWMpA2pukMk3S9ppaQVkoYl5b+W9HxqvzF5zsHMzJrqnlfHkroB04GTgAZgkaS5EbGisU1EXJ5q/zVgbKqL3wDXRcQDkvYB3k/VXRkRtXmN3czMWpbnEcd4oD4iVkXEu8As4IydtD8fmAkgaQTQPSIeAIiItyJiU45jNTOzVsozOAYBq1PbDUnZDiQNBaqAh5OijwIbJN0l6a+SfpQcwTS6TtLS5FRXrxb6nCKpTlLdunXr2j4bMzMDOs7ieA1QGxHbku3uwLHAFcCRwKHA5KTuamB4Uj4AuKpUhxExIyIKEVGorKzMcehmZl1LnsGxBhiS2h6clJVSQ3KaKtEALElOc20F7gbGAUTES1H0DnArxVNiZma2m+QZHIuAwyRVSepJMRzmNm8kaThQASxstm9/SY2HCicAK5L2ByVfBUwEluU1ATMz21FuV1VFxFZJlwLzgG7ALRGxXNK1QF1ENIZIDTArIiK17zZJVwAPJQGxGLgpqb49CRQBS4Cv5DUHMzPbkVKf13usQqEQdXV15R6GmVmnImlxRBSal3eUxXEzM+skHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMWhUckr4uaV8V3SzpcUkn5z04MzPreFp7xPHPEfEmcDLFGxJ+Abg+t1GZmVmH1drgUPL1VOC/ImJ5qszMzLqQ1gbHYkn3UwyOeZL60fQZ4GZm1kW09rbqXwTGAKsiYpOkAcDFuY3KzMw6rNYecXwCeDoiNki6EPgu8EZ+wzIzs46qtcHxC2CTpI8D3wKeA36T26jMzKzDam1wbE2e0HcG8LOImA70y29YZmbWUbV2jWOjpKspXoZ7rKS9gB75DcvMzDqq1h5xTALeofj3HC8Dg4Ef5TYqMzPrsFoVHElY3A7sJ+lzwJaI8BqHmVkX1NpbjpwH/AU4FzgPeEzSOXkOzMzMOqbWrnF8BzgyIl4FkFQJPAjU5jUwMzPrmFq7xrFXY2gk1rdmX0kTJD0tqV7S1BL10yQtSV7PSNqQqjtE0v2SVkpaIWlYUl4l6bGkzzsk9WzlHMzMrB20Njj+n6R5kiZLmgz8Hrh3ZztI6gZMB04BRgDnSxqRbhMRl0fEmIgYA/wUuCtV/RvgRxFRDYwHGoPrB8C0iPgI8DrFv2o3M7PdpLWL41cCM4DRyWtGRFz1AbuNB+ojYlVEvAvMovh3IC05H5gJkARM94h4IPn+byW3OhFwAn8/RXYbMLE1czAzs/bR2jUOIuJO4M4MfQ8CVqe2G4CjSjWUNBSoAh5Oij4KbJB0V1L+IDCV4i3dN0TE1lSfg1rocwowBeCQQw7JMGwzM9uZnR5xSNoo6c0Sr42S3mzHcdQAtRGxLdnuDhwLXAEcCRwKTM7SYUTMiIhCRBQqKyvbcahmZl3bTo84IqIttxVZAwxJbQ9OykqpAb6a2m4AlkTEKgBJdwNHA7cA/SV1T446dtanmZnlIM9nji8CDkuugupJMRzmNm8kaTjFU1ALm+3bP7nsF4rrGiuS+2XNBxr/huQi4J6cxm9mZiXkFhzJEcGlwDxgJTA7IpZLulbS6ammNcCsJBQa991G8TTVQ5KepPi0wZuS6quAb0qqBwYCN+c1BzMz25FSn9d7rEKhEHV1deUehplZpyJpcUQUmpfnearKzMz2QA4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0xyDQ5JEyQ9Lale0tQS9dMkLUlez0jakKrblqqbmyr/taTnU3Vj8pyDmZk11T2vjiV1A6YDJwENwCJJcyNiRWObiLg81f5rwNhUF5sjYkwL3V8ZEbXtP2ozM/sgeR5xjAfqI2JVRLwLzALO2En784GZOY7HzMzaQZ7BMQhYndpuSMp2IGkoUAU8nCruLalO0qOSJjbb5TpJS5NTXb1a6HNKsn/dunXrdn0WZmbWREdZHK8BaiNiW6psaEQUgM8DN0r6cFJ+NTAcOBIYAFxVqsOImBERhYgoVFZW5jh0M7OuJc/gWAMMSW0PTspKqaHZaaqIWJN8XQUsIFn/iIiXougd4FaKp8TMzGw3yTM4FgGHSaqS1JNiOMxt3kjScKACWJgqq2g8BSVpf+AYYEWyfVDyVcBEYFmOczAzs2Zyu6oqIrZKuhSYB3QDbomI5ZKuBeoiojFEaoBZERGp3auBX0l6n2K4XZ+6Gut2SZWAgCXAV/Kag5mZ7UhNP6/3TIVCIerq6so9DDOzTkXS4mStuYmOsjhuZmadhIPDzMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLBMHh5mZZeLgMDOzTBwcZmaWiYPDzMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLBMHh5mZZeLgMDOzTBwcZmaWiYPDzMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLJNcg0PSBElPS6qXNLVE/TRJS5LXM5I2pOq2permpsqrJD2W9HmHpJ55zsHMzJrKLTgkdQOmA6cAI4DzJY1It4mIyyNiTESMAX4K3JWq3txYFxGnp8p/AEyLiI8ArwNfzGsOZma2ozyPOMYD9RGxKiLeBWYBZ+yk/fnAzJ11KEnACUBtUnQbMLHtQzUzs9bKMzgGAatT2w1J2Q4kDQWqgIdTxb0l1Ul6VNLEpGwgsCEitraizynJ/nXr1q1rwzTMzCyte7kHkKgBaiNiW6psaESskXQo8LCkJ4E3WtthRMwAZgAUCoVo19GamXVheR5xrAGGpLYHJ2Wl1NDsNFVErEm+rgIWAGOB9UB/SY2Bt7M+zcwsB3kGxyLgsOQqqJ4Uw2Fu80aShgMVwMJUWYWkXsn7/YFjgBUREcB84Jyk6UXAPTnOwczMmsktOJJ1iEuBecBKYHZELJd0raT0VVI1wKwkFBpVA3WSnqAYFNdHxIqk7irgm5LqKa553JzXHMzMbEdq+nm9ZyoUClFXV1fuYZiZdSqSFkdEoXm5/3LczMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLBMHh5mZZeLgMDOzTBwcZmaWiYPDzMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLBMHh5mZZeLgMDOzTBwcZmaWiYPDzMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLJNcg0PSBElPS6qXNLVE/TRJS5LXM5I2NKvfV1KDpJ+lyhYkfTbud0CeczAzs6a659WxpG7AdOAkoAFYJGluRKxobBMRl6fafw0Y26yb/wD+UKL7CyKirv1HbWZmHyTPI47xQH1ErIqId4FZwBk7aX8+MLNxQ9IRwIHA/TmO0czMMsozOAYBq1PbDUnZDiQNBaqAh5PtvYD/BVzRQt+3Jqep/k2S2m/IZmb2QTrK4ngNUBsR25LtfwXujYiGEm0viIjDgWOT1xdKdShpiqQ6SXXr1q3LZdBmZl1RnsGxBhiS2h6clJVSQ+o0FfAJ4FJJLwA/Bv5J0vUAEbEm+boR+C3FU2I7iIgZEVGIiEJlZWVb5mFmZim5LY4Di4DDJFVRDIwa4PPNG0kaDlQACxvLIuKCVP1koBARUyV1B/pHxGuSegCfAx7McQ5mZtZMbsEREVslXQrMA7oBt0TEcknXAnURMTdpWgPMiohoRbe9gHlJaHSjGBo35TB8MzNrgVr3ed25FQqFqKvz1btmZllIWhwRheblHWVx3MzMOokuccQhaR3wYrnHkdH+wGvlHsRu5jl3DZ5z5zE0Ina4uqhLBEdnJKmu1CHinsxz7ho8587Pp6rMzCwTB4eZmWXi4Oi4ZpR7AGXgOXcNnnMn5zUOMzPLxEccZmaWiYPDzMwycXCUkaQBkh6Q9GzytaKFdhclbZ6VdFGJ+rmSluU/4rZry5wl9ZH0e0lPSVreeOPLjqoVT8DsJemOpP4xScNSdVcn5U9L+sfdOvA22NU5SzpJ0mJJTyZfT9jtg99Fbfl3TuoPkfSWpJYeI9HxRIRfZXoBPwSmJu+nAj8o0WYAsCr5WpG8r0jVn0XxLsHLyj2fvOcM9AGOT9r0BP4InFLuObUwz27Ac8ChyVifAEY0a/OvwC+T9zXAHcn7EUn7XhSfU/Mc0K3cc8p5zmOBg5P3o4A15Z5P3nNO1dcCvwOuKPd8WvvyEUd5nQHclry/DZhYos0/Ag9ExN8i4nXgAWACgKR9gG8C389/qO1ml+ccEZsiYj5AFJ8q+TjF2/V3RK15Amb6Z1ELnJg8mOwMijf+fCcingfqaeHxAR3MLs85Iv4aEWuT8uXA3pJ67ZZRt01b/p2RNBF4nuKcOw0HR3kdGBEvJe9fpvio3OZ29iTF/6D4pMRNuY2w/bV1zgBI6g+cBjyUwxjbQ2uegLm9TURsBd4ABrZy346oLXNOOxt4PCLeyWmc7WmX55z84ncV8O+7YZztKs/ncRgg6UHgQyWqvpPeiIiQ1OproyWNAT4cEZc3P2dabnnNOdV/d4oP/vpJRKzatVFaRyRpJPAD4ORyj2U3uAaYFhFvdbYnYDs4chYRn2mpTtIrkg6KiJckHQS8WqLZGuC41PZgYAHFpyQWkqckdgcOkLQgIo6jzHKcc6MZwLMRcWPbR5ub1jwBs7FNQxKG+wHrW7lvR9SWOSNpMDAH+KeIeC7/4baLtsz5KOAcST8E+gPvS9oSET/LfdRtVe5Flq78An5E04XiH5ZoM4DiOdCK5PU8MKBZm2F0nsXxNs2Z4nrOncBe5Z7LB8yzO8VF/Sr+vmg6slmbr9J00XR28n4kTRfHV9E5FsfbMuf+Sfuzyj2P3TXnZm2uoRMtjpd9AF35RfHc7kPAsxSfZtj44VgA/neq3T9TXCCtBy4u0U9nCo5dnjPF3+YCWAksSV5fKvecdjLXU4FnKF51852k7Frg9OR9b4pX09QDfwEOTe37nWS/p+mgV46155yB7wJvp/5dlwAHlHs+ef87p/roVMHhW46YmVkmvqrKzMwycXCYmVkmDg4zM8vEwWFmZpk4OMzMLBMHh1kHJ+k4Sf+33OMwa+TgMDOzTBwcZu1E0oWS/iJpiaRfSeqWPGdhWvL8kIckVSZtx0h6VNJSSXMan0si6SOSHpT0hKTHJX046X4fSbXJs0hub7y7qlk5ODjM2oGkamAScExEjAG2ARcAfYG6iBgJPAJ8L9nlN8BVETEaeDJVfjswPSI+DvwD0Hgn4bHANyg+q+NQ4Jicp2TWIt/k0Kx9nAgcASxKDgb2pngDx/eBO5I2/we4S9J+QP+IeCQpvw34naR+wKCImAMQEVsAkv7+EhENyfYSireZ+VPuszIrwcFh1j4E3BYRVzcplP6tWbtdvcdP+tkU2/D/XSsjn6oyax8PUbxF9gGw/dnqQyn+HzsnafN54E8R8QbwuqRjk/IvAI9ExEaKt96emPTRS1Kf3TkJs9bwby1m7SAiVkj6LnC/pL2A9yjeTvttYHxS9yrFdRCAi4BfJsGwCrg4Kf8C8CtJ1yZ9nLsbp2HWKr47rlmOJL0VEfuUexxm7cmnqszMLBMfcZiZWSY+4jAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPL5P8DjFYKWXUKwbwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=1, batch_size=50)\n",
    "\n",
    "print(history.history.keys())\n",
    "# Plot of accuracy in each epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# Plot of loss in each epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test,y_test)\n",
    "print(\"Test set accuracy: \",acc)\n",
    "print(\"Test set loss: \", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the model in a hdf5 file\n",
    "model.save('models/english_review_epoch1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from saved hdf5 file\n",
    "model = load_model('models/english_review_epoch1.h5',custom_objects={'AttentionLayer': AttentionLayer})\n",
    "\n",
    "# articleDB = pd.DataFrame(columns = ['Title','Summary','Text','Category','Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article=review_df[:5]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean article with pre described rules\n",
    "article_cleaned,idx_list = cleanString(article,stopWords)\n",
    "input_array = wordToSeq(article_cleaned,word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_att_weights = Model(inputs=sent_input,outputs=sent_coeffs)\n",
    "output_array = sent_att_weights.predict(np.resize(input_array,(1,MAX_SENTENCE_NUM,MAX_WORD_NUM)))\n",
    "\n",
    "# Get n sentences with most attention in document\n",
    "n_sentences = 5\n",
    "sent_index = output_array.flatten().argsort()[-n_sentences:]\n",
    "sent_index = np.sort(sent_index)\n",
    "sent_index = sent_index.tolist()\n",
    "\n",
    "# Create summary using n sentences\n",
    "sent_list = tokenize.sent_tokenize(article)\n",
    "summary = [sent_list[i] for i in sent_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
