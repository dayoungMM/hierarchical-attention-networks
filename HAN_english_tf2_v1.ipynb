{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import initializers as initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      5  I have bought several of the Vitality canned d...\n",
       "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2      4  This is a confection that has been around a fe...\n",
       "3      2  If you are looking for the secret ingredient i...\n",
       "4      5  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_review = pd.read_csv('dataset/Reviews.csv')\n",
    "df_review = df_review[['Score','Text']]\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(df_review['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dayoung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dayoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dayoung/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have bought several of vitality canned dog food product and have found them all to be of good quality . product look more like stew than processed meat and it smell better . my labrador is finicky and she appreciates this product better than most . '"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean String\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleanString(review,stopWords):\n",
    "    \"\"\"\n",
    "    Cleans input string using set rules.\n",
    "    Cleaning rules:         Every word is lemmatized and lowercased. Stopwords and non alpha-numeric words are removed.\n",
    "                            Each sentence ends with a period.\n",
    "    Input:   review       - string(in sentence structure)\n",
    "             stopWords    - set of strings which should be removed from review\n",
    "    Output:  returnString - cleaned input string\n",
    "             idx_list     - list of lists, one list is equal to one sentence. In every list are the index\n",
    "                            of each word as they appeared in the non cleaned sentence\n",
    "                            e.g. nonCleaned = \"This is a test.\" -> cleaned = \"This test.\" -> cleaned_index = [[0,3]]\n",
    "    \"\"\"\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.sent_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' . '\n",
    "    \n",
    "    return returnString, idx_list\n",
    "\n",
    "\n",
    "cleanString(df_review['Text'][0],['a','the'])[0]\n",
    "# print(df_review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleanString 한 결과 저장해놓은 것\n",
    "df_review = pd.read_csv('dataset/df_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:20: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<ipython-input-39-07520ecccf60>:20: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  s = ' '.join([word.strip(string.punctuation) for word in s.split() if word.strip(string.punctuation) is not \"\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 100040\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from tensorflow import keras\n",
    "\n",
    "# Parameters\n",
    "MAX_SENTENCE_NUM = 9\n",
    "MAX_WORD_NUM = 40\n",
    "MAX_FEATURES = 200000 \n",
    "\n",
    "# Tokenization\n",
    "# Word index\n",
    "\n",
    "\"\"\"\n",
    "Using the keras Tokenizer class a word index is built.\n",
    "The most 'MAX_FEATURES' used words are tokenized to a number.\n",
    "this dictionary is saved in word_index\n",
    "\"\"\"\n",
    "texts = []\n",
    "for i in range(len(df_review)):\n",
    "    s = df_review['Text'].iloc[i]\n",
    "    s = ' '.join([word.strip(string.punctuation) for word in s.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    texts.append(s)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_FEATURES,lower=True, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Number of tokens: ' + str(len(word_index)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_review['Text']=df_review['Text'].apply(lambda x : cleanString(x,['a','the'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Total absent words are 13065 which is 13.06 % of total words\n",
      "Words with 2 or less mentions 52105 which is 52.08 % of total words\n",
      "34870 words to proceed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "A pre-trained word to vector is used from GloVe by Pennington et. al.\n",
    "Source: https://nlp.stanford.edu/projects/glove/\n",
    "The data was trained on wikipedia articles. Each word is described by a 100d vector.\n",
    "\"\"\"\n",
    "\n",
    "# Load word vectors from pre-trained dataset\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(os.getcwd(), 'dataset/glove.6B.100d.txt'), encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Embedding\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "\n",
    "min_wordCount = 2\n",
    "absent_words = 0\n",
    "small_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "word_counts = tokenizer.word_counts\n",
    "for word, i in word_index.items():\n",
    "    if word_counts[word] > min_wordCount:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            absent_words += 1\n",
    "    else:\n",
    "        small_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)),\n",
    "      '% of total words')\n",
    "print('Words with '+str(min_wordCount)+' or less mentions', small_words, 'which is', \"%0.2f\" % (small_words * 100 / len(word_index)),\n",
    "      '% of total words')\n",
    "print(str(len(word_index)-small_words-absent_words) + ' words to proceed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
       "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
       "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
       "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
       "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
       "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
       "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
       "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
       "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
       "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
       "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
       "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
       "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
       "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
       "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
       "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
       "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
       "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
       "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
       "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display(example_df_embedding)\n",
    "embedding_matrix[word_index['great']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_review.to_csv('dataset/df_review.csv',index=False)\n",
    "# import tensorflow as tf\n",
    "# import pandas as pd\n",
    "# df_review = pd.read_csv('dataset/df_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_WORD_LENGTH = 7\n",
    "# MAX_WORDS = 10\n",
    "# MAX_NB_CHARS = 1000\n",
    "# EMBEDDING_DIM = 10\n",
    "# VALIDATION_SPLIT = 0.2\n",
    "\n",
    "## Parameters\n",
    "# MAX_SENTENCE_NUM = 9\n",
    "# MAX_WORD_NUM = 40\n",
    "# MAX_FEATURES = 200000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<ipython-input-49-f0b0aee0edbb>:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  sentences = text.apply(lambda x: [word.strip(string.punctuation) for word in x.split() if word.strip(string.punctuation) is not \"\"])\n"
     ]
    }
   ],
   "source": [
    "text = df_review['Text']\n",
    "\n",
    "sentences = text.apply(lambda x: [word.strip(string.punctuation) for word in x.split() if word.strip(string.punctuation) is not \"\"])\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=MAX_FEATURES, char_level=True)\n",
    "# tokenizer.fit_on_texts(sentences.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(sentences), MAX_SENTENCE_NUM, MAX_WORD_NUM ), dtype='int32')\n",
    "\n",
    "for i, words in enumerate(sentences):\n",
    "    for j, word in enumerate(words):\n",
    "        if j < MAX_SENTENCE_NUM:\n",
    "            k = 0\n",
    "            for _, char in enumerate(word):\n",
    "                try:\n",
    "                    if k < MAX_WORD_NUM:\n",
    "                        if tokenizer.word_index[char] < MAX_FEATURES:\n",
    "                            data[i, j, k] = tokenizer.word_index[char]\n",
    "                            k=k+1\n",
    "                except:\n",
    "                    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 100040 unique tokens.\n",
      "Shape of data tensor: (568454, 9, 40)\n",
      "Shape of label tensor: (568454, 5)\n"
     ]
    }
   ],
   "source": [
    "char_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(char_index))\n",
    "\n",
    "labels = pd.get_dummies(df_review['Score']).values\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[ 47075  26835  38328  72490 326881]\n",
      "[ 5193  2934  4312  8165 36241]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(0.1 * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print (y_train.sum(axis=0))\n",
    "print (y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = df_review['Text'].apply(lambda x: tokenize.sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[i have bought several of vitality canned dog ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[product arrived labeled a jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[this is confection that ha been around few ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[if you are looking for secret ingredient in r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[great taffy at great price ., there wa wide a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      5  [i have bought several of vitality canned dog ...\n",
       "1      1  [product arrived labeled a jumbo salted peanut...\n",
       "2      4  [this is confection that ha been around few ce...\n",
       "3      2  [if you are looking for secret ingredient in r...\n",
       "4      5  [great taffy at great price ., there wa wide a..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_review['Text']=temp\n",
    "# df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_FEATURES = 200000 # maximum number of unique words that should be included in the tokenized word index\n",
    "# MAX_SENTENCE_NUM = 40 # maximum number of sentences in one document\n",
    "# MAX_WORD_NUM = 50     # maximum number of words in each sentence\n",
    "# EMBED_SIZE = 100      # vector size of word embedding\n",
    "\n",
    "MAX_SENTENCE_NUM = 9\n",
    "MAX_WORD_NUM = 40\n",
    "MAX_FEATURES = 200000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  568454\n",
      "Training:  454763 , Percentage:  0.7999996481685413\n",
      "Validation:  56846 , Percentage:  0.10000105549437598\n",
      "Test: 56845 , Percentage:  0.09999929633708268\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def split_df(dataframe, column_name, training_split, validation_split, test_split):\n",
    "    \"\"\"\n",
    "    Splits a pandas dataframe into trainingset, validationset and testset in specified ratio.\n",
    "    All sets are balanced, which means they have the same ratio for each category as the full set.\n",
    "    Input:   dataframe        - Pandas Dataframe, should include a column for data and one for categories\n",
    "             column_name      - Name of dataframe column which contains the categorical output values\n",
    "             training_split   - from ]0,1[, default = 0.6\n",
    "             validation_split - from ]0,1[, default = 0.2        \n",
    "             test_split       - from ]0,1[, default = 0.2\n",
    "                                Sum of all splits need to be 1\n",
    "    Output:  train            - Pandas DataFrame of trainset\n",
    "             validation       - Pandas DataFrame of validationset\n",
    "             test             - Pandas DataFrame of testset\n",
    "    \"\"\"\n",
    "    if training_split + validation_split + test_split != 1.0:\n",
    "        raise ValueError('Split paramter sum should be 1.0')\n",
    "        \n",
    "    total = len(dataframe.index)\n",
    " \n",
    "    train = dataframe.reset_index().groupby(column_name).apply(lambda x: x.sample(frac=training_split))\\\n",
    "    .reset_index(drop=True).set_index('index')\n",
    "    train = train.sample(frac=1)\n",
    "    temp_df = dataframe.drop(train.index)\n",
    "    validation = temp_df.reset_index().groupby(column_name)\\\n",
    "    .apply(lambda x: x.sample(frac=validation_split/(test_split+validation_split)))\\\n",
    "           .reset_index(drop=True).set_index('index')\n",
    "    validation = validation.sample(frac=1)\n",
    "    test = temp_df.drop(validation.index)\n",
    "    test = test.sample(frac=1)\n",
    "    \n",
    "    print('Total: ', len(dataframe))\n",
    "    print('Training: ', len(train), ', Percentage: ', len(train)/len(dataframe))\n",
    "    print('Validation: ', len(validation), ', Percentage: ', len(validation)/len(dataframe))\n",
    "    print('Test:', len(test), ', Percentage: ', len(test)/len(dataframe))\n",
    "\n",
    "    return train, validation, test\n",
    "\n",
    "train, validation, test = split_df(df_review, 'Score',0.8,0.1,0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i have bought several of vitality canned dog food product and have found them all to be of good quality .',\n",
       "       'product look more like stew than processed meat and it smell better .',\n",
       "       'my labrador is finicky and she appreciates this product better than most .'],\n",
       "      dtype='<U105')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(train['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teeeeemp = train['Text'].apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "K = tf.keras.backend\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n",
    "    - Yang et. al.\n",
    "    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "    Theano backend\n",
    "    \"\"\"\n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self._trainable_weights = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input (InputLayer)      [(None, 40)]              0         \n",
      "_________________________________________________________________\n",
      "word_embedding (Embedding)   (None, 40, 100)           10004100  \n",
      "_________________________________________________________________\n",
      "word_gru (Bidirectional)     (None, 40, 100)           45600     \n",
      "_________________________________________________________________\n",
      "word_dense (Dense)           (None, 40, 100)           10100     \n",
      "_________________________________________________________________\n",
      "word_attention (AttentionLay [(None, 100), (None, 40,  10200     \n",
      "=================================================================\n",
      "Total params: 10,070,000\n",
      "Trainable params: 10,070,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      [(None, 9, 40)]           0         \n",
      "_________________________________________________________________\n",
      "sent_linking (TimeDistribute (None, 9, 100)            10070000  \n",
      "_________________________________________________________________\n",
      "sent_gru (Bidirectional)     (None, 9, 100)            45600     \n",
      "_________________________________________________________________\n",
      "sent_dense (Dense)           (None, 9, 100)            10100     \n",
      "_________________________________________________________________\n",
      "sent_attention (AttentionLay [(None, 100), (None, 9, 1 10200     \n",
      "_________________________________________________________________\n",
      "sent_dropout (Dropout)       (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 10,136,405\n",
      "Trainable params: 10,136,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import initializers\n",
    "\"\"\"\n",
    "Create Keras functional model for hierarchical attention network\n",
    "\"\"\"\n",
    "# original embedding layer\n",
    "# embedding_layer = Embedding(len(word_index) + 1,EMBED_SIZE,weights=[embedding_matrix], \n",
    "#                             input_length=MAX_WORD_NUM, trainable=False,name='word_embedding')\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    embedding_layer = Embedding(len(char_index) + 1,\n",
    "                                EMBED_SIZE,#10\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_WORD_NUM,\n",
    "                                trainable=True,\n",
    "                               name='word_embedding')\n",
    "    # Words level attention model\n",
    "    word_input = Input(shape=(MAX_WORD_NUM,), dtype='int32',name='word_input')\n",
    "    word_sequences = embedding_layer(word_input)\n",
    "    word_gru = Bidirectional(GRU(50, return_sequences=True),name='word_gru')(word_sequences)\n",
    "    word_dense = Dense(100, activation='relu', name='word_dense')(word_gru) \n",
    "    word_att,word_coeffs = AttentionLayer(EMBED_SIZE,True,name='word_attention')(word_dense)\n",
    "    wordEncoder = Model(inputs = word_input,outputs = word_att)\n",
    "\n",
    "    # Sentence level attention model\n",
    "    sent_input = Input(shape=(MAX_SENTENCE_NUM,MAX_WORD_NUM), dtype='int32',name='sent_input')\n",
    "    sent_encoder = TimeDistributed(wordEncoder,name='sent_linking')(sent_input)\n",
    "    sent_gru = Bidirectional(GRU(50, return_sequences=True),name='sent_gru')(sent_encoder)\n",
    "    sent_dense = Dense(100, activation='relu', name='sent_dense')(sent_gru) \n",
    "    sent_att,sent_coeffs = AttentionLayer(EMBED_SIZE,return_coefficients=True,name='sent_attention')(sent_dense)\n",
    "    sent_drop = Dropout(0.5,name='sent_dropout')(sent_att)\n",
    "    preds = Dense(5, activation='softmax',name='output')(sent_drop)\n",
    "\n",
    "    # Model compile\n",
    "    model = Model(sent_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "    print(wordEncoder.summary())\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = train['Score']\n",
    "# # x_train = train['Text']\n",
    "# x_train = teeeeemp\n",
    "# y_val = validation['Score']\n",
    "# x_val = validation['Text']\n",
    "# y_test = test['Score']\n",
    "# x_test = test['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.shape\n",
    "# y_train = to_categorical(np.asarray(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sequences = tokenizer.texts_to_sequences(x_train)\n",
    "# x_train = pad_sequences(sequences, maxlen=MAX_WORD_NUM)\n",
    "# sequences = tokenizer.texts_to_sequences(x_test)\n",
    "# x_test = pad_sequences(sequences, maxlen=MAX_WORD_NUM)\n",
    "# sequences = tokenizer.texts_to_sequences(x_val)\n",
    "# x_val = pad_sequences(sequences, maxlen=MAX_WORD_NUM)\n",
    "\n",
    "# y_train = to_categorical(np.asarray(y_train))\n",
    "# y_val = to_categorical(np.asarray(y_val))\n",
    "# y_test = to_categorical(np.asarray(y_test))\n",
    "# sequences = tokenizer.texts_to_sequences(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = data[:-nb_validation_samples]\n",
    "# y_train = labels[:-nb_validation_samples]\n",
    "# x_val = data[-nb_validation_samples:]\n",
    "# y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 511609 samples\n",
      "391392/511609 [=====================>........] - ETA: 14:17 - loss: 1.0528 - acc: 0.6452"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x_train, y_train, epochs=1, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 134/1777 [=>............................] - ETA: 1:26 - loss: 1.0037 - acc: 0.6455"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-f96da345d7b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/detect/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.evaluate(x=x_val, y=y_val, batch_size=None, verbose=1, sample_weight=None, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean article with pre described rules\n",
    "article_cleaned,idx_list = cleanString(article,stopWords)\n",
    "input_array = wordToSeq(article_cleaned,word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_att_weights = Model(inputs=sent_input,outputs=sent_coeffs)\n",
    "output_array = sent_att_weights.predict(np.resize(input_array,(1,MAX_SENTENCE_NUM,MAX_WORD_NUM)))\n",
    "\n",
    "# Get n sentences with most attention in document\n",
    "n_sentences = 5\n",
    "sent_index = output_array.flatten().argsort()[-n_sentences:]\n",
    "sent_index = np.sort(sent_index)\n",
    "sent_index = sent_index.tolist()\n",
    "\n",
    "# Create summary using n sentences\n",
    "sent_list = tokenize.sent_tokenize(article)\n",
    "summary = [sent_list[i] for i in sent_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.046353  , 0.03741193, 0.06129214, 0.14663391, 0.70830905],\n",
       "       [0.10746627, 0.08834272, 0.11369296, 0.17507371, 0.5154243 ],\n",
       "       [0.2514229 , 0.22567882, 0.2718035 , 0.12152804, 0.12956679],\n",
       "       ...,\n",
       "       [0.0696284 , 0.0417748 , 0.05960848, 0.13166185, 0.6973265 ],\n",
       "       [0.12501307, 0.07824251, 0.08861791, 0.14852275, 0.55960375],\n",
       "       [0.03537109, 0.03469939, 0.06586904, 0.1752103 , 0.6888502 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = model.predict(np.expand_dims(input_array,axis=0))\n",
    "res = model.predict(x_val)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pd.Series(map(np.argmax,res))\n",
    "y_true = pd.Series(map(np.argmax,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6578942739027179"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred == y_true)/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56840</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56841</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56842</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56843</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56844</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56845 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0      4  4\n",
       "1      4  1\n",
       "2      2  1\n",
       "3      4  4\n",
       "4      4  4\n",
       "...   .. ..\n",
       "56840  4  3\n",
       "56841  0  2\n",
       "56842  4  0\n",
       "56843  4  4\n",
       "56844  4  4\n",
       "\n",
       "[56845 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.Series(y_pred),pd.Series(y_true)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
